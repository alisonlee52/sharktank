# -*- coding: utf-8 -*-
"""nb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RnO6nJWJXimK26_2mVTHcZUzUn0xBmLl
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, ConfusionMatrixDisplay
import math
from sklearn.linear_model import SGDClassifier, LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from scipy import stats
sns.set() # use seaborn plotting style

#for question specific models
mydata= pd.read_csv("/content/drive/MyDrive/Junior Year/IW10/Data Analysys/questions - Sheet1.csv")
print("question-specific testing")
questionsx = mydata["Question"]
gendery = mydata["M/F"]
#idk if this is right
classification_tags=["Male", "Female"]

#for pitch specific models
mydata= pd.read_csv("/content/drive/MyDrive/Junior Year/IW10/Data Analysys/questionsbypitch - Sheet2.csv")
print("pitch-specific model")
questionsx = mydata["Whole Text"]
gendery = mydata["M/F"]
classification_tags=["Male", "Female"]

# setting the test and non
def set_test():
  X = []
  y = gendery
  #for random forest
  y=y.map({"Male":0,"Female":1})
  ####
  vectorizer = CountVectorizer()
  X = vectorizer.fit_transform(questionsx)
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) #can add , random_state = 42

  #print("model built using "+ str(X_train.shape[0])+ " observations")
  #print("model tested using "+ str(X_test.shape[0])+ " observations")

  return X_train, X_test, y_train, y_test, vectorizer

# see top female words and top male words

X_train, X_test, y_train, y_test, vectorizer = set_test()
mnb = MultinomialNB()
mnb.fit(X_train, y_train)
pred = mnb.predict(X_test)

#print(classification_report(y_test, pred))
#print(mnb.get_params())
#print(y_test)

#labels = [0,1]
#cm = confusion_matrix(y_test, pred, labels=labels)
#disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
#disp.plot();  

male_class_prob_sorted = mnb.feature_log_prob_[0, :].argsort()[::-1]
female_class_prob_sorted = mnb.feature_log_prob_[1, :].argsort()[::-1]

## men
# male_class_prob_sorted.tofile('men_words.csv', sep = ',')
#female_class_prob_sorted.tofile('women_words.csv', sep = ',')

#print(np.take(vectorizer.get_feature_names_out(), male_class_prob_sorted[:]))
#print(np.take(vectorizer.get_feature_names_out(), female_class_prob_sorted[:200]))

#print(female_class_prob_sorted.size)


for i in range(female_class_prob_sorted.size):
  f = open("women.txt", "a")
  f.write(np.take(vectorizer.get_feature_names_out(), female_class_prob_sorted[i]))
  f.write(",")
  f.close()

for i in range(male_class_prob_sorted.size):
  f = open("men.txt", "a")
  f.write(np.take(vectorizer.get_feature_names_out(), male_class_prob_sorted[i]))
  f.write(",")
  f.close()

accuracy = sum(1 for i in range(pred.shape[0]) if pred[i] == y_test.values[i]) / float(pred.shape[0])

#print(pred)
#print(y_test)

#print(type(pred))
#print(type(y_test))
#print(pred.shape)
#print(y_test.shape)

y_testt = y_test.to_numpy()
#print(y_testt)

##print(type(pred))
#print(type(y_testt))
#print(pred.shape)
#print(y_testt.shape)

i=0
count=0
for index, value in y_test.items():
  if value!=pred[i]:
    print(questionsx[index])
    count+=1
  i+=1

print("The number of incorrectly-identified datapoints are: ", count)

# naive bayes
def naive_bayes(X_train, X_test, y_train, y_test, vectorizer):
  mnb = MultinomialNB()
  mnb.fit(X_train, y_train)
  pred = mnb.predict(X_test)

  #print(classification_report(y_test, pred))
  #print(mnb.get_params())
  #print(y_test)

  #labels = [0,1]
  #cm = confusion_matrix(y_test, pred, labels=labels)
  #disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
  #disp.plot();  

  neg_class_prob_sorted = mnb.feature_log_prob_[0, :].argsort()[::-1]
  pos_class_prob_sorted = mnb.feature_log_prob_[1, :].argsort()[::-1]

  #print(np.take(vectorizer.get_feature_names_out(), neg_class_prob_sorted[:100]))
  #print(np.take(vectorizer.get_feature_names_out(), pos_class_prob_sorted[:100]))

  accuracy = sum(1 for i in range(pred.shape[0]) if pred[i] == y_test.values[i]) / float(pred.shape[0])
  return accuracy



# male and female are equal, meaning balanced dataset (would use F-score for unbalanced dataset) SHOULD JUSTIFY THIS IN REPORT


  ## RESULT FOR QUESTION-SPECIFIC: better than a coin flip = {.48, .56} 
  ## RESULT FOR PITCH-SPECIFIC: little better than a coin flip = {.32, .7}



# naive bayes
def naive_bayes(X_train, X_test, y_train, y_test):
  mnb = MultinomialNB()
  mnb.fit(X_train, y_train)
  pred = mnb.predict(X_test)

  accuracy = sum(1 for i in range(pred.shape[0]) if pred[i] == y_test.values[i]) / float(pred.shape[0])
  return accuracy

set_test()

#SGD classifier
def sgd(X_train, X_test, y_train, y_test):
  sgd = Pipeline([('tfidf', TfidfTransformer()),
                  ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),
                ])
  sgd.fit(X_train, y_train)


  y_pred = sgd.predict(X_test)
  accuracy = accuracy_score(y_test, y_pred)
  return accuracy

  #print('accuracy %s' % accuracy_score(y_pred, y_test))
  #print(classification_report(y_test, y_pred))

  ## RESULT FOR QUESTION-SPECIFIC: worse than a coin flip = {.47, .52}
  ## RESULT FOR PITCH-SPECIFIC: little better than a coin flip = {.34, .52} :(

def logreg(X_train, X_test, y_train, y_test):
  logreg = Pipeline([('tfidf', TfidfTransformer()),
                  ('clf', LogisticRegression(n_jobs=1, C=1e5, max_iter=5000)),
                ])
  logreg.fit(X_train, y_train)
  y_pred = logreg.predict(X_test)
  accuracy = accuracy_score(y_test, y_pred)
  return accuracy

  #print('accuracy %s' % accuracy_score(y_pred, y_test))
  #print(classification_report(y_test, y_pred))
 ## RESULT FOR QUESTION-SPECIFIC: worse than a coin flip = {.489, .55}
  ## RESULT FOR PITCH-SPECIFIC: little better than a coin flip = {.30, .73}

#random forest
def random_forest(X_train, X_test, y_train, y_test):
  rf = RandomForestClassifier()
  rf.fit(X_train, y_train)
  y_pred = rf.predict(X_test)
  accuracy = accuracy_score(y_test, y_pred)
  return accuracy
  #print("Accuracy:", accuracy)

## RESULT FOR QUESTION-SPECIFIC: better than a coin flip = {.51, .559}
## RESULT FOR PITCH-SPECIFIC: little better than a coin flip = {.43, .69}

# testing

alpha = 0.05
first_test =[]
second_test=[]

for i in range(100):
  X_train, X_test, y_train, y_test, vectorizer = set_test()
  first_test.append(naive_bayes(X_train, X_test, y_train, y_test))
  second_test.append(sgd(X_train, X_test, y_train, y_test))

t_value,p_value=stats.ttest_rel(first_test,second_test)

one_tailed_p_value=float("{:.6f}".format(p_value/2)) 

print('Test statistic is %f'%float("{:.6f}".format(t_value)))

print('p-value for one_tailed_test is %f'%one_tailed_p_value)

if one_tailed_p_value<=alpha:

    print('Conclusion','n','Since p-value(=%f)'%one_tailed_p_value,'<','alpha(=%.2f)'%alpha,'''We reject the null hypothesis H0. 

So we conclude that these predictors are different and the second is better. i.e., d = 0 at %.2f level of significance.'''%alpha)

else:

    print('Conclusion','n','Since p-value(=%f)'%one_tailed_p_value,'>','alpha(=%.2f)'%alpha,'''We do not reject the null hypothesis H0. 

So we conclude that these two predictors do not differ in accuracy. i.e., d = 0 at %.2f level of significance.'''%alpha)
    

# For Naive Bayes, Random Forest: 
#### Test statistic is 0.643764
#p-value for one_tailed_test is 0.260609
###Conclusion n Since p-value(=0.260609) > alpha(=0.05) We do not reject the null hypothesis H0. 
#So we conclude that these two predictors do not differ in accuracy. i.e., d = 0 at 0.05 level of significance.


# For Random Forest, Naive Bayes: 
##Test statistic is -2.811055
##p-value for one_tailed_test is 0.002977
##Conclusion n Since p-value(=0.002977) < alpha(=0.05) We reject the null hypothesis H0. 
##So we conclude that these predictors are different and the second is better. i.e., d = 0 at 0.05 level of significance.

## Naive Bayes is better!! IS BEST!!

alpha = 0.05
dif_test =[]

for i in range(100):
  X_train, X_test, y_train, y_test, vectorizer = set_test()
  dif_test.append(logreg(X_train, X_test, y_train, y_test)-naive_bayes(X_train, X_test, y_train, y_test))

stats.t.interval(confidence=0.95, df=len(dif_test)-1, loc=np.mean(dif_test), scale=stats.sem(dif_test))

test =[]

for i in range(100):
  X_train, X_test, y_train, y_test, vectorizer = set_test()
  test.append(logreg(X_train, X_test, y_train, y_test))

stats.t.interval(confidence=0.95, df=len(test)-1, loc=np.mean(test), scale=stats.sem(test))